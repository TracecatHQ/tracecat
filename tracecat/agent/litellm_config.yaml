# LiteLLM Proxy Configuration
# Credentials are NOT stored here - they're injected per-request via custom_auth

model_list:
  # OpenAI models
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini

  - model_name: gpt-5-mini
    litellm_params:
      model: openai/gpt-5-mini

  - model_name: gpt-5-nano
    litellm_params:
      model: openai/gpt-5-nano

  - model_name: gpt-5
    litellm_params:
      model: openai/gpt-5

  # Anthropic models
  - model_name: claude-sonnet-4-5-20250929
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929

  - model_name: claude-haiku-4-5-20251001
    litellm_params:
      model: anthropic/claude-haiku-4-5-20251001

  - model_name: claude-opus-4-5-20251101
    litellm_params:
      model: anthropic/claude-opus-4-5-20251101

  # Bedrock wildcard - catches dynamically-set ARN models (bedrock/arn:aws:...)
  - model_name: bedrock/*
    litellm_params:
      model: bedrock/*

  # Azure OpenAI wildcard - catches azure/<deployment-name> models
  - model_name: azure/*
    litellm_params:
      model: azure/*

  # Azure AI wildcard - catches azure_ai/<model-name> models
  - model_name: azure_ai/*
    litellm_params:
      model: azure_ai/*

  # Catch-all wildcard for any model not explicitly defined, for custom models
  - model_name: "*"
    litellm_params:
      model: openai/*

general_settings:
  custom_auth: tracecat.agent.gateway.user_api_key_auth
  database_url: null

litellm_settings:
  callbacks:
    - tracecat.agent.gateway.callback_handler
  drop_params: true
